"""
NASDAQ Composite Sentiment Tracker - Hybrid Approach
Tracks sentiment across top 20 NASDAQ stocks + general market news
Run with: python manage.py run_nasdaq_sentiment
"""

import os
import time
import hashlib
import math
from datetime import datetime, timedelta
from decimal import Decimal
from django.core.management.base import BaseCommand
from django.utils import timezone
import pytz
from django.db.models import Avg, Min, Max, Count, Sum
from django.db import transaction
import requests
import finnhub
from dotenv import load_dotenv

# Load environment variables from project root
load_dotenv(os.path.join(os.path.dirname(__file__), '..', '..', '..', '..', '.env'))

# Import models
from api.models import Ticker, AnalysisRun, NewsArticle, SentimentHistory, TickerContribution

# Import NASDAQ configuration
from .nasdaq_config import (
    NASDAQ_TOP_20,
    COMPANY_NAMES,
    MARKET_MOVING_KEYWORDS,
    EXCLUDE_KEYWORDS,
    SOURCE_CREDIBILITY,
    SENTIMENT_WEIGHTS,
    API_RATE_LIMIT_DELAY,
    INDICATOR_SYMBOLS,
    INDICATOR_RESOLUTION,
    INDICATOR_LOOKBACK_HOURS,
    INDICATOR_PERIODS,
)

# Import technical indicators calculator (uses Yahoo Finance OHLCV + math)
from .technical_indicators import fetch_indicators_with_fallback, fetch_latest_ohlcv_from_yfinance, calculate_technical_composite_score, fetch_vxn_price

# Import Reddit sentiment analysis
from .reddit_fetcher import fetch_all_reddit_content
from .reddit_sentiment_analyzer import analyze_reddit_content_batch

# Import market hours checker
from api.utils.market_hours import is_market_open, get_market_status

# ============================================================================
# CONFIGURATION
# ============================================================================

FINNHUB_API_KEY = os.environ.get('FINNHUB_API_KEY', '')
HUGGINGFACE_API_KEY = os.environ.get('HUGGINGFACE_API_KEY', '')

# Weights for composite score calculation (within each article)
# Updated: 70% sentiment, 15% surprise, 15% credibility (removed novelty and recency)
ARTICLE_WEIGHTS = {
    'base_sentiment': 0.70,
    'surprise_factor': 0.15,
    'source_credibility': 0.15
}

# Cache to track seen articles in current session
seen_articles = set()

# ============================================================================
# HELPER FUNCTIONS
# ============================================================================

def get_article_hash(headline, summary):
    """Generate unique hash for article deduplication"""
    content = f"{headline}{summary}".lower()
    return hashlib.md5(content.encode()).hexdigest()


def get_cached_sentiment_from_db(article_hash):
    """Get cached sentiment from database"""
    try:
        article = NewsArticle.objects.filter(article_hash=article_hash).first()
        if article and article.is_analyzed:
            return article.base_sentiment
    except Exception as e:
        print(f"‚ö†Ô∏è Error fetching cached sentiment: {e}")
    return None


def analyze_sentiment_finbert_api(text):
    """Analyze sentiment using FinBERT via HuggingFace API (single text)"""
    API_URL = "https://api-inference.huggingface.co/models/ProsusAI/finbert"
    headers = {"Authorization": f"Bearer {HUGGINGFACE_API_KEY}"}
    text = text[:512]
    
    try:
        response = requests.post(API_URL, headers=headers, json={"inputs": text}, timeout=30)

        if response.status_code == 200:
            result = response.json()
            sentiment_map = {'positive': 0, 'negative': 0, 'neutral': 0}
            
            for item in result[0]:
                label = item['label'].lower()
                score = item['score']
                sentiment_map[label] = score
            
            sentiment_score = sentiment_map['positive'] - sentiment_map['negative']
            return sentiment_score
            
        elif response.status_code == 503:
            print("  ‚è≥ Model loading, retrying in 20 seconds...")
            time.sleep(20)
            return analyze_sentiment_finbert_api(text)
        else:
            print(f"  ‚ö†Ô∏è API error: {response.status_code}")
            return 0.0
            
    except requests.exceptions.Timeout:
        print(f"  ‚è±Ô∏è  API timeout after 30s - returning neutral sentiment")
        return 0.0
    except Exception as e:
        print(f"  ‚ö†Ô∏è Error analyzing sentiment: {e}")
        return 0.0


def analyze_sentiment_finbert_batch(texts, max_retries=2):
    """
    Analyze sentiment for multiple texts in a single batch API call with retry logic
    Returns: list of sentiment scores in same order as input texts (None = timeout/error, retry next run)

    Args:
        texts: List of text strings to analyze
        max_retries: Number of retry attempts for timeouts (default: 2)
    """
    API_URL = "https://api-inference.huggingface.co/models/ProsusAI/finbert"
    headers = {"Authorization": f"Bearer {HUGGINGFACE_API_KEY}"}

    # Truncate each text to 512 characters
    truncated_texts = [text[:512] for text in texts]

    for attempt in range(max_retries + 1):
        try:
            # Increase timeout on retries (30s, 45s, 60s)
            timeout = 30 + (attempt * 15)

            if attempt > 0:
                print(f"  üîÑ Retry attempt {attempt}/{max_retries} with {timeout}s timeout...")

            # Send batch request
            response = requests.post(API_URL, headers=headers, json={"inputs": truncated_texts}, timeout=timeout)

            if response.status_code == 200:
                results = response.json()
                sentiment_scores = []

                # Process each result
                for result in results:
                    sentiment_map = {'positive': 0, 'negative': 0, 'neutral': 0}

                    for item in result:
                        label = item['label'].lower()
                        score = item['score']
                        sentiment_map[label] = score

                    sentiment_score = sentiment_map['positive'] - sentiment_map['negative']
                    sentiment_scores.append(sentiment_score)

                if attempt > 0:
                    print(f"  ‚úÖ Retry successful!")

                return sentiment_scores

            elif response.status_code == 503:
                print("  ‚è≥ Model loading, retrying in 20 seconds...")
                time.sleep(20)
                continue  # Retry without counting against max_retries
            else:
                print(f"  ‚ö†Ô∏è Batch API error: {response.status_code} - will retry these articles next run")
                # Return None for all texts (don't cache errors)
                return [None] * len(texts)

        except requests.exceptions.Timeout:
            if attempt < max_retries:
                wait_time = 5 * (attempt + 1)  # 5s, 10s backoff
                print(f"  ‚è±Ô∏è  API timeout after {timeout}s - waiting {wait_time}s before retry...")
                time.sleep(wait_time)
                continue  # Retry
            else:
                print(f"  ‚è±Ô∏è  API timeout after {max_retries} retries - will retry {len(texts)} articles next run (not caching)")
                return [None] * len(texts)  # ‚úÖ Return None instead of 0.0
        except Exception as e:
            print(f"  ‚ö†Ô∏è Error in batch sentiment analysis: {e} - will retry next run")
            return [None] * len(texts)

    # Should never reach here, but safety fallback
    return [None] * len(texts)


def calculate_surprise_factor(text):
    """Detect if news is surprising/unexpected"""
    text_lower = text.lower()
    
    surprise_keywords = {
        'unexpected': 1.5, 'surprise': 1.5, 'beats expectations': 1.8,
        'misses estimates': 1.8, 'exceeds expectations': 1.8, 'shock': 2.0,
        'unprecedented': 1.7, 'sudden': 1.4, 'abrupt': 1.4, 'breaking': 1.3
    }
    
    expected_keywords = {
        'as expected': 0.4, 'in line with': 0.4, 'anticipated': 0.5,
        'scheduled': 0.6, 'planned': 0.6
    }
    
    multiplier = 1.0
    for keyword, weight in surprise_keywords.items():
        if keyword in text_lower:
            multiplier = max(multiplier, weight)
    for keyword, weight in expected_keywords.items():
        if keyword in text_lower:
            multiplier = min(multiplier, weight)
    
    return multiplier


def calculate_novelty(article_hash):
    """Check if article is novel or duplicate"""
    if article_hash in seen_articles:
        return 0.2
    seen_articles.add(article_hash)
    return 1.0


def get_source_credibility(source):
    """Get credibility weight for news source"""
    for known_source, weight in SOURCE_CREDIBILITY.items():
        if known_source.lower() in source.lower():
            return weight
    return 0.5


def calculate_recency_weight(published_timestamp):
    """Calculate weight based on how recent the article is"""
    try:
        published_time = datetime.fromtimestamp(published_timestamp)
        hours_old = (datetime.now() - published_time).total_seconds() / 3600
        decay_factor = 2 ** (-hours_old / 6)
        return max(0, min(1.0, decay_factor))
    except:
        return 0.5


def apply_news_decay(previous_score, minutes_since_update=1):
    """
    Apply exponential decay to news_composite score
    Score decays to near-zero over 120 minutes (2 hours)

    Args:
        previous_score: The news_composite score from previous run
        minutes_since_update: Number of minutes since last update (default 1)

    Returns:
        Decayed score (moving toward zero)
    """
    if previous_score == 0:
        return 0.0

    # Exponential decay rate that reaches ~1% after 120 minutes
    # Using 3.83% decay per minute
    decay_rate = 0.0383

    # Apply decay (works same for positive and negative scores - both move toward zero)
    decayed_score = previous_score * ((1 - decay_rate) ** minutes_since_update)

    # Force to zero if very small (within 0.01 of zero)
    if abs(decayed_score) < 0.01:
        return 0.0

    return decayed_score


def sanitize_nan(value, default=0.0):
    """
    Sanitize NaN and Inf values to prevent database storage issues.
    Converts NaN/Inf to the specified default value (usually 0).

    Args:
        value: The value to sanitize
        default: Default value to use if NaN/Inf detected (default: 0.0)

    Returns:
        Sanitized value (original if valid, default if NaN/Inf/None)
    """
    if value is None:
        return default

    try:
        float_value = float(value)
        # Check for NaN or Infinity
        if math.isnan(float_value) or math.isinf(float_value):
            print(f"  ‚ö†Ô∏è Detected NaN/Inf value, converting to {default}")
            return default
        return value
    except (TypeError, ValueError):
        return default


# ============================================================================
# NEWS FETCHING FUNCTIONS
# ============================================================================

def fetch_company_news_batch(finnhub_client, tickers, lookback_hours=24):
    """
    Fetch news for multiple tickers with rate limiting
    Returns: dict mapping ticker -> list of news articles
    """
    print(f"\nüì∞ Fetching company news for {len(tickers)} tickers...")
    
    to_date = datetime.now()
    from_date = to_date - timedelta(hours=lookback_hours)
    from_str = from_date.strftime('%Y-%m-%d')
    to_str = to_date.strftime('%Y-%m-%d')
    
    all_news = {}
    
    for ticker in tickers:
        try:
            print(f"  Fetching {ticker}...", end=' ')
            news = finnhub_client.company_news(ticker, _from=from_str, to=to_str)
            all_news[ticker] = news
            print(f"‚úì {len(news)} articles")
            
            # Rate limiting to avoid hitting API limits
            time.sleep(API_RATE_LIMIT_DELAY)
            
        except Exception as e:
            print(f"‚ùå Error: {e}")
            all_news[ticker] = []
    
    total_articles = sum(len(articles) for articles in all_news.values())
    print(f"‚úÖ Total company news articles fetched: {total_articles}")
    
    return all_news


def fetch_general_market_news(finnhub_client):
    """
    Fetch general market news and filter for market-moving content
    Returns: list of relevant market news articles
    """
    print(f"\nüìà Fetching general market news...")
    
    try:
        # Fetch general news - using 'general' category
        news = finnhub_client.general_news('general', min_id=0)
        
        print(f"  Received {len(news)} general news articles")
        
        # Filter for market-moving news
        relevant_news = []
        
        for article in news:
            headline = article.get('headline', '').lower()
            summary = article.get('summary', '').lower()
            combined_text = f"{headline} {summary}"
            
            # Check if article contains market-moving keywords
            has_market_keyword = any(
                keyword in combined_text 
                for keyword in MARKET_MOVING_KEYWORDS
            )
            
            # Check if article should be excluded (opinion pieces, etc.)
            has_exclude_keyword = any(
                keyword in combined_text 
                for keyword in EXCLUDE_KEYWORDS
            )
            
            if has_market_keyword and not has_exclude_keyword:
                relevant_news.append(article)
        
        print(f"‚úÖ Filtered to {len(relevant_news)} market-moving articles")
        return relevant_news
        
    except Exception as e:
        print(f"‚ùå Error fetching general market news: {e}")
        return []


# ============================================================================
# SENTIMENT ANALYSIS FUNCTIONS
# ============================================================================

def analyze_article_sentiment(article, ticker_obj, article_type='company', base_sentiment=None):
    """
    Analyze sentiment for a single article and return all metrics
    If base_sentiment is provided, skip FinBERT analysis (used in batch processing)
    Returns: dict with all sentiment components
    """
    headline = article.get('headline', '')
    summary = article.get('summary', '')
    source = article.get('source', 'Unknown')
    url = article.get('url', '')
    published_at = article.get('datetime', int(time.time()))
    
    # Generate unique hash
    article_hash = get_article_hash(headline, summary)
    
    # Check if already analyzed or if sentiment was provided
    if base_sentiment is None:
        cached_sentiment = get_cached_sentiment_from_db(article_hash)
        is_cached = cached_sentiment is not None
        
        # Analyze sentiment
        if is_cached:
            base_sentiment = cached_sentiment
        else:
            combined_text = f"{headline}. {summary}"
            base_sentiment = analyze_sentiment_finbert_api(combined_text)
    else:
        # Sentiment was provided from batch processing
        is_cached = False
    
    # Calculate all component factors (removed novelty and recency)
    surprise_factor = calculate_surprise_factor(f"{headline} {summary}")
    source_credibility = get_source_credibility(source)

    # Calculate article score using amplified weighted formula
    # AMPLIFIED SCORING: 3x multipliers for better visibility and movement
    # Base sentiment: -250 to +250 (was -70 to +70)
    # Surprise factor: 0 to +50 (was -7.5 to +37.5)
    # Source credibility: -25 to +25 (was 0 to +3)
    # Typical range: -325 to +325 (before market cap weighting)
    article_score = (
        base_sentiment * ARTICLE_WEIGHTS['base_sentiment'] * 250 +
        (surprise_factor - 1) * ARTICLE_WEIGHTS['surprise_factor'] * 150 +
        (source_credibility - 0.5) * ARTICLE_WEIGHTS['source_credibility'] * 50
    )

    # Convert timestamp to timezone-aware datetime
    published_datetime = datetime.fromtimestamp(published_at, tz=pytz.UTC)

    return {
        'headline': headline,
        'summary': summary,
        'source': source,
        'url': url,
        'published_at': published_datetime,
        'article_hash': article_hash,
        'article_type': article_type,
        'base_sentiment': base_sentiment,
        'surprise_factor': surprise_factor,
        'novelty_score': 0.0,  # No longer used (set to 0.0 for database compatibility)
        'source_credibility': source_credibility,
        'recency_weight': 0.0,  # No longer used (set to 0.0 for database compatibility)
        'article_score': article_score,
        'is_cached': is_cached
    }


def analyze_articles_batch(articles, ticker_obj, article_type='company'):
    """
    Analyze sentiment for multiple articles using batch processing
    Returns: list of article data dicts
    """
    if not articles:
        return []
    
    # Separate cached and uncached articles
    articles_to_analyze = []
    cached_articles = []
    article_hashes = []
    
    for article in articles:
        headline = article.get('headline', '')
        summary = article.get('summary', '')
        article_hash = get_article_hash(headline, summary)
        article_hashes.append(article_hash)
        
        cached_sentiment = get_cached_sentiment_from_db(article_hash)
        if cached_sentiment is not None:
            # Article already analyzed
            cached_articles.append((article, cached_sentiment))
        else:
            # Need to analyze
            articles_to_analyze.append(article)
    
    # Batch process uncached articles
    new_sentiments = []
    if articles_to_analyze:
        print(f"    üî¨ Analyzing {len(articles_to_analyze)} new articles with FinBERT (batched)...")
        texts = [f"{a.get('headline', '')}. {a.get('summary', '')}" for a in articles_to_analyze]
        new_sentiments = analyze_sentiment_finbert_batch(texts)
    
    # Process all articles
    all_articles_data = []
    
    # Add cached articles
    for article, sentiment in cached_articles:
        article_data = analyze_article_sentiment(article, ticker_obj, article_type, base_sentiment=sentiment)
        all_articles_data.append(article_data)
    
    # Add newly analyzed articles
    for article, sentiment in zip(articles_to_analyze, new_sentiments):
        article_data = analyze_article_sentiment(article, ticker_obj, article_type, base_sentiment=sentiment)
        all_articles_data.append(article_data)
    
    if articles_to_analyze:
        print(f"    ‚úÖ Batch analysis complete ({len(cached_articles)} cached, {len(articles_to_analyze)} new)")
    
    return all_articles_data


def analyze_ticker_sentiment(finnhub_client, ticker_symbol, ticker_obj, lookback_hours=24):
    """
    Analyze sentiment for a single ticker's news
    Returns: tuple (sentiment_score, article_count, articles_data)
    """
    # Fetch news
    to_date = datetime.now()
    from_date = to_date - timedelta(hours=lookback_hours)
    from_str = from_date.strftime('%Y-%m-%d')
    to_str = to_date.strftime('%Y-%m-%d')
    
    try:
        news = finnhub_client.company_news(ticker_symbol, _from=from_str, to=to_str)
    except Exception as e:
        print(f"  ‚ùå Error fetching news for {ticker_symbol}: {e}")
        return 0.0, 0, []
    
    if not news:
        return 0.0, 0, []
    
    # Analyze all articles
    articles_data = []
    total_score = 0
    
    for article in news:
        article_data = analyze_article_sentiment(article, ticker_obj, article_type='company')
        articles_data.append(article_data)
        total_score += article_data['article_score']
    
    # Calculate average sentiment score
    avg_sentiment = total_score / len(articles_data) if articles_data else 0
    
    return avg_sentiment, len(articles_data), articles_data


# ============================================================================
# ANALYST RECOMMENDATIONS FUNCTIONS
# ============================================================================

def check_if_new_recommendations_available(finnhub_client, sample_symbols=None):
    """
    Check if new recommendations are available by sampling a few stocks
    Returns True if new recommendations are found, False if same as last run
    """
    if sample_symbols is None:
        # Sample first 5 stocks to check for updates
        sample_symbols = list(NASDAQ_TOP_20.keys())[:5]
    
    print(f"  üîç Checking for new recommendations (sampling {len(sample_symbols)} stocks)...")
    
    # Get the latest analysis run to compare against
    latest_run = AnalysisRun.objects.filter(ticker__symbol='^IXIC').order_by('-timestamp').first()
    
    if not latest_run or latest_run.analyst_recommendations_count == 0:
        print(f"    No previous recommendations found - fetching all")
        return True
    
    # Check sample stocks for changes
    new_data_found = False
    
    for symbol in sample_symbols:
        try:
            recommendations = finnhub_client.recommendation_trends(symbol)
            if recommendations and len(recommendations) > 0:
                latest = recommendations[0]
                total_recommendations = (
                    latest.get('strongBuy', 0) + 
                    latest.get('buy', 0) + 
                    latest.get('hold', 0) + 
                    latest.get('sell', 0) + 
                    latest.get('strongSell', 0)
                )
                
                if total_recommendations > 0:
                    # Check if this is significantly different from what we have
                    # For now, just check if the total count has changed
                    # In a more sophisticated version, we could check the actual recommendation breakdown
                    if total_recommendations != (latest_run.analyst_recommendations_count // len(NASDAQ_TOP_20)):
                        new_data_found = True
                        break
                        
        except Exception as e:
            print(f"    ‚ö†Ô∏è Error checking {symbol}: {e}")
            continue
    
    if new_data_found:
        print(f"    ‚úÖ New recommendations detected - fetching all")
    else:
        print(f"    üìä No new recommendations - using cached data")
    
    return new_data_found


def fetch_analyst_recommendations(finnhub_client):
    """
    Fetch analyst recommendations for NASDAQ top 20 stocks and aggregate them
    Returns weighted average recommendation score (-100 to +100)
    """
    print(f"  üìä Fetching analyst recommendations for {len(NASDAQ_TOP_20)} stocks...")
    
    total_strong_buy = 0
    total_buy = 0
    total_hold = 0
    total_sell = 0
    total_strong_sell = 0
    total_weighted_score = 0.0
    total_weight = 0.0
    
    stocks_with_recommendations = 0
    
    for symbol, market_cap_weight in NASDAQ_TOP_20.items():
        try:
            # Fetch recommendations for this stock
            recommendations = finnhub_client.recommendation_trends(symbol)
            
            if recommendations and len(recommendations) > 0:
                latest = recommendations[0]
                
                # Get recommendation counts
                strong_buy = latest.get('strongBuy', 0)
                buy = latest.get('buy', 0)
                hold = latest.get('hold', 0)
                sell = latest.get('sell', 0)
                strong_sell = latest.get('strongSell', 0)
                
                total_recommendations = strong_buy + buy + hold + sell + strong_sell
                
                if total_recommendations > 0:
                    # Calculate weighted score for this stock (-1 to +1 range)
                    # Strong Buy=2, Buy=1, Hold=0, Sell=-1, Strong Sell=-2
                    weighted_score = (
                        strong_buy * 2 + 
                        buy * 1 + 
                        hold * 0 + 
                        sell * -1 + 
                        strong_sell * -2
                    )
                    
                    # Normalize to -1 to +1 range
                    normalized_score = weighted_score / (total_recommendations * 2)
                    
                    # Apply market cap weighting
                    total_weighted_score += normalized_score * market_cap_weight
                    total_weight += market_cap_weight
                    
                    # Add to totals
                    total_strong_buy += strong_buy
                    total_buy += buy
                    total_hold += hold
                    total_sell += sell
                    total_strong_sell += strong_sell
                    
                    stocks_with_recommendations += 1
                    
                    print(f"    {symbol}: {strong_buy}S+{buy}B+{hold}H+{sell}S+{strong_sell}SS (score: {normalized_score:+.2f})")
                else:
                    print(f"    {symbol}: No recommendation data")
            else:
                print(f"    {symbol}: No recommendations available")
                
        except Exception as e:
            print(f"    {symbol}: Error - {e}")
            continue
    
    # Calculate final composite score (-100 to +100)
    if total_weight > 0:
        composite_score = (total_weighted_score / total_weight) * 100
    else:
        composite_score = 0.0
    
    total_recommendations = total_strong_buy + total_buy + total_hold + total_sell + total_strong_sell
    
    return {
        'composite_score': composite_score,
        'total_recommendations': total_recommendations,
        'strong_buy': total_strong_buy,
        'buy': total_buy,
        'hold': total_hold,
        'sell': total_sell,
        'strong_sell': total_strong_sell,
        'stocks_analyzed': stocks_with_recommendations,
        'total_stocks': len(NASDAQ_TOP_20)
    }


# ============================================================================
# MAIN ANALYSIS FUNCTIONS
# ============================================================================

def run_nasdaq_composite_analysis(finnhub_client):
    """
    Main function to run complete NASDAQ composite analysis
    Uses hybrid approach: company news (70%) + market news (30%)
    """
    print("\n" + "="*80)
    print("üöÄ STARTING NASDAQ COMPOSITE SENTIMENT ANALYSIS")
    print("="*80)
    
    start_time = time.time()
    
    # Step 1: Initialize or get NASDAQ composite ticker
    nasdaq_ticker, created = Ticker.objects.get_or_create(
        symbol='^IXIC',
        defaults={
            'company_name': 'NASDAQ Composite Index',
            'exchange': 'NASDAQ'
        }
    )
    if created:
        print(f"‚ú® Created NASDAQ composite ticker: ^IXIC")
    
    # Step 2: Initialize all component tickers
    print(f"\nüìä Initializing {len(NASDAQ_TOP_20)} component tickers...")
    ticker_objects = {}
    for symbol, weight in NASDAQ_TOP_20.items():
        ticker_obj, _ = Ticker.objects.get_or_create(
            symbol=symbol,
            defaults={'company_name': COMPANY_NAMES.get(symbol, f'{symbol} Inc.')}
        )
        ticker_objects[symbol] = ticker_obj
    print(f"‚úÖ All tickers initialized")
    
    # Step 3: Fetch company news for all tickers with rate limiting
    print(f"\nüì∞ PHASE 1: Fetching company-specific news")
    print(f"   Weight in composite: {SENTIMENT_WEIGHTS['company_news']:.0%}")
    company_news_dict = fetch_company_news_batch(
        finnhub_client, 
        list(NASDAQ_TOP_20.keys()),
        lookback_hours=24
    )
    
    # Step 3.5: Check if we have any new articles
    print(f"\nüîç Checking for new articles...")
    has_new_articles = False
    all_article_hashes = []
    
    # Collect all article hashes from fetched news
    for symbol, news_articles in company_news_dict.items():
        for article in news_articles[:10]:  # Check top 10 per ticker
            article_hash = get_article_hash(
                article.get('headline', ''),
                article.get('summary', '')
            )
            all_article_hashes.append(article_hash)
    
    # Also check general market news
    market_news_preview = fetch_general_market_news(finnhub_client)
    for article in market_news_preview[:20]:  # Check top 20 market articles
        article_hash = get_article_hash(
            article.get('headline', ''),
            article.get('summary', '')
        )
        all_article_hashes.append(article_hash)
    
    # Check if any articles are new (not in database)
    if all_article_hashes:
        existing_hashes = set(
            NewsArticle.objects.filter(
                article_hash__in=all_article_hashes
            ).values_list('article_hash', flat=True)
        )
        new_hashes = set(all_article_hashes) - existing_hashes
        has_new_articles = len(new_hashes) > 0
        
        print(f"   Total articles found: {len(all_article_hashes)}")
        print(f"   Already analyzed: {len(existing_hashes)}")
        print(f"   New articles: {len(new_hashes)}")

    # Step 3.5: Fetch and analyze Reddit content (runs every time, regardless of new articles)
    print(f"\nüî¥ PHASE 4: Analyzing Reddit sentiment")
    reddit_sentiment = 0.0
    reddit_analysis_data = {}

    try:
        # Fetch Reddit posts and comments
        reddit_content = fetch_all_reddit_content()

        if reddit_content and reddit_content['posts']:
            # Analyze Reddit content
            reddit_analysis_data = analyze_reddit_content_batch(
                reddit_content['posts'],
                reddit_content['comments'],
                nasdaq_ticker
            )

            reddit_sentiment = reddit_analysis_data['composite_score']
            print(f"‚úÖ Reddit Sentiment: {reddit_sentiment:+.2f}")
            print(f"   Posts analyzed: {reddit_analysis_data.get('posts_analyzed', 0)}")
            print(f"   Comments analyzed: {reddit_analysis_data.get('comments_analyzed', 0)}")
        else:
            print(f"  ‚ö†Ô∏è  No Reddit content fetched (check API credentials)")

    except Exception as e:
        print(f"  ‚ö†Ô∏è  Reddit analysis failed: {e}")
        import traceback
        traceback.print_exc()
        reddit_sentiment = 0.0

    # Step 4.5: Fetch and analyze Analyst Recommendations
    print(f"\nüìä PHASE 5: Analyzing Analyst Recommendations")
    analyst_recommendations_score = 0.0
    analyst_recommendations_data = {}

    try:
        # Check if new recommendations are available before fetching all
        if check_if_new_recommendations_available(finnhub_client):
            analyst_recommendations_data = fetch_analyst_recommendations(finnhub_client)
            analyst_recommendations_score = analyst_recommendations_data['composite_score']
            print(f"‚úÖ Analyst Recommendations: {analyst_recommendations_score:+.2f}")
            print(f"   Total recommendations: {analyst_recommendations_data.get('total_recommendations', 0)}")
            print(f"   Strong Buy: {analyst_recommendations_data.get('strong_buy', 0)}")
            print(f"   Buy: {analyst_recommendations_data.get('buy', 0)}")
            print(f"   Hold: {analyst_recommendations_data.get('hold', 0)}")
            print(f"   Sell: {analyst_recommendations_data.get('sell', 0)}")
            print(f"   Strong Sell: {analyst_recommendations_data.get('strong_sell', 0)}")
        else:
            # Use cached data from latest run
            latest_run = AnalysisRun.objects.filter(ticker=nasdaq_ticker).order_by('-timestamp').first()
            if latest_run and latest_run.analyst_recommendations_score is not None:
                analyst_recommendations_score = latest_run.analyst_recommendations_score
                analyst_recommendations_data = {
                    'composite_score': latest_run.analyst_recommendations_score,
                    'total_recommendations': latest_run.analyst_recommendations_count,
                    'strong_buy': latest_run.analyst_strong_buy,
                    'buy': latest_run.analyst_buy,
                    'hold': latest_run.analyst_hold,
                    'sell': latest_run.analyst_sell,
                    'strong_sell': latest_run.analyst_strong_sell
                }
                print(f"‚úÖ Analyst Recommendations (cached): {analyst_recommendations_score:+.2f}")
                print(f"   Total recommendations: {analyst_recommendations_data.get('total_recommendations', 0)}")
                print(f"   Strong Buy: {analyst_recommendations_data.get('strong_buy', 0)}")
                print(f"   Buy: {analyst_recommendations_data.get('buy', 0)}")
                print(f"   Hold: {analyst_recommendations_data.get('hold', 0)}")
                print(f"   Sell: {analyst_recommendations_data.get('sell', 0)}")
                print(f"   Strong Sell: {analyst_recommendations_data.get('strong_sell', 0)}")
            else:
                print(f"  ‚ö†Ô∏è  No cached analyst recommendations found")
                analyst_recommendations_score = 0.0
    except Exception as e:
        print(f"  ‚ö†Ô∏è  Analyst recommendations analysis failed: {e}")
        import traceback
        traceback.print_exc()
        analyst_recommendations_score = 0.0

    # If no new articles, create a duplicate run with updated price
    if not has_new_articles:
        print(f"\n‚úÖ No new articles found - creating new run with updated price")

        # Get latest analysis run
        latest_run = AnalysisRun.objects.filter(ticker=nasdaq_ticker).order_by('-timestamp').first()

        if latest_run:
            # Fetch current stock price and OHLCV from Yahoo Finance
            try:
                ohlcv = fetch_latest_ohlcv_from_yfinance(symbol='^IXIC', interval='1m')

                if ohlcv:
                    new_price = Decimal(str(ohlcv['close']))
                    new_open = Decimal(str(ohlcv['open']))
                    new_high = Decimal(str(ohlcv['high']))
                    new_low = Decimal(str(ohlcv['low']))
                    new_volume = ohlcv['volume']
                    new_change = ((ohlcv['close'] - ohlcv['open']) / ohlcv['open'] * 100) if ohlcv['open'] != 0 else 0
                else:
                    # Fallback to Finnhub
                    quote = finnhub_client.quote('^IXIC')
                    new_price = Decimal(str(quote['c']))
                    new_open = Decimal(str(quote.get('o', quote['c'])))
                    new_high = Decimal(str(quote.get('h', quote['c'])))
                    new_low = Decimal(str(quote.get('l', quote['c'])))
                    new_change = quote.get('dp', 0)
                    new_volume = None

                # Calculate fresh technical indicators from database history
                tech_indicators, symbol_used = fetch_indicators_with_fallback(
                    symbols=INDICATOR_SYMBOLS,
                    resolution=INDICATOR_RESOLUTION,
                    hours_back=INDICATOR_LOOKBACK_HOURS,
                    config=INDICATOR_PERIODS
                )

                # Calculate technical composite score with error handling
                try:
                    tech_composite = calculate_technical_composite_score(tech_indicators)
                    print(f"‚úÖ Technical composite calculated: {tech_composite}")
                except Exception as tech_error:
                    print(f"‚ö†Ô∏è  ERROR calculating technical composite: {tech_error}")
                    import traceback
                    traceback.print_exc()
                    tech_composite = 0.0  # Fallback to 0

                # Fetch VXN (NASDAQ-100 Volatility Index)
                decay_vxn_value = fetch_vxn_price()

                # Recalculate composite score with 4-factor model
                # Apply decay to news, use fresh reddit, technical, and analyst recommendations
                NEWS_WEIGHT = 0.35
                SOCIAL_WEIGHT = 0.20
                TECHNICAL_WEIGHT = 0.25
                ANALYST_WEIGHT = 0.20

                # Extract news sentiment from previous run and apply decay
                previous_news_composite = latest_run.avg_base_sentiment or 0.0

                # Calculate time elapsed since last update (in minutes)
                time_elapsed = (timezone.now() - latest_run.timestamp).total_seconds() / 60
                minutes_elapsed = max(1, int(time_elapsed))  # At least 1 minute

                # Apply decay to previous score (no new articles)
                news_composite = apply_news_decay(previous_news_composite, minutes_elapsed)

                # Cap at -100/+100
                news_composite = max(-100, min(100, news_composite))

                print(f"\nüìâ Applying decay (no new articles):")
                print(f"   Previous score: {previous_news_composite:+.2f}")
                print(f"   Minutes elapsed: {minutes_elapsed}")
                print(f"   Decayed score: {news_composite:+.2f}")

                # Calculate NEW composite score with 4-factor model
                new_composite_score = (
                    news_composite * NEWS_WEIGHT +
                    reddit_sentiment * SOCIAL_WEIGHT +
                    tech_composite * TECHNICAL_WEIGHT +
                    analyst_recommendations_score * ANALYST_WEIGHT
                )

                # Determine sentiment label
                if new_composite_score >= 30:
                    sentiment_label = 'BULLISH'
                elif new_composite_score <= -30:
                    sentiment_label = 'BEARISH'
                else:
                    sentiment_label = 'NEUTRAL'

                print(f"\nüìä RECALCULATED COMPOSITE SCORE (Cached Articles Path):")
                print(f"   News Sentiment:            {news_composite:+.2f} √ó {NEWS_WEIGHT:.0%} = {news_composite * NEWS_WEIGHT:+.2f}")
                print(f"   Social Media (Reddit):     {reddit_sentiment:+.2f} √ó {SOCIAL_WEIGHT:.0%} = {reddit_sentiment * SOCIAL_WEIGHT:+.2f}")
                print(f"   Technical Indicators:      {tech_composite:+.2f} √ó {TECHNICAL_WEIGHT:.0%} = {tech_composite * TECHNICAL_WEIGHT:+.2f}")
                print(f"   Analyst Recommendations:   {analyst_recommendations_score:+.2f} √ó {ANALYST_WEIGHT:.0%} = {analyst_recommendations_score * ANALYST_WEIGHT:+.2f}")
                print(f"   Final Composite:           {new_composite_score:+.2f} ({sentiment_label})")

                # Create a new run with recalculated composite and updated price
                new_run = AnalysisRun.objects.create(
                    ticker=nasdaq_ticker,
                    composite_score=new_composite_score,
                    sentiment_label=sentiment_label,
                    stock_price=new_price,
                    price_open=new_open,
                    price_high=new_high,
                    price_low=new_low,
                    price_change_percent=new_change,
                    volume=new_volume,
                    avg_base_sentiment=news_composite,  # Save the decayed news_composite, not the old one
                    avg_surprise_factor=latest_run.avg_surprise_factor,
                    avg_novelty=latest_run.avg_novelty,
                    avg_source_credibility=latest_run.avg_source_credibility,
                    avg_recency_weight=latest_run.avg_recency_weight,
                    articles_analyzed=latest_run.articles_analyzed,
                    cached_articles=latest_run.articles_analyzed,
                    new_articles=0,
                    # Copy technical indicators from the most recent run that has them (with MACD calculation)
                    rsi_14=tech_indicators['rsi_14'],
                    macd=tech_indicators['macd'],
                    macd_signal=tech_indicators['macd_signal'],
                    macd_histogram=tech_indicators['macd_histogram'],
                    bb_upper=tech_indicators['bb_upper'],
                    bb_middle=tech_indicators['bb_middle'],
                    bb_lower=tech_indicators['bb_lower'],
                    sma_20=tech_indicators['sma_20'],
                    sma_50=tech_indicators['sma_50'],
                    ema_9=tech_indicators['ema_9'],
                    ema_20=tech_indicators['ema_20'],
                    stoch_k=tech_indicators['stoch_k'],
                    stoch_d=tech_indicators['stoch_d'],
                    williams_r=tech_indicators['williams_r'],
                    atr_14=tech_indicators['atr_14'],
                    qqq_price=tech_indicators['qqq_price'],
                    # VXN (NASDAQ-100 Volatility Index)
                    vxn_index=decay_vxn_value,
                    # Reddit sentiment (fresh analysis even in cached path)
                    reddit_sentiment=reddit_sentiment,
                    reddit_posts_analyzed=reddit_analysis_data.get('posts_analyzed', 0),
                    reddit_comments_analyzed=reddit_analysis_data.get('comments_analyzed', 0),
                    # Technical composite score
                    technical_composite_score=tech_composite,
                    # Analyst recommendations
                    analyst_recommendations_score=analyst_recommendations_score,
                    analyst_recommendations_count=analyst_recommendations_data.get('total_recommendations', 0),
                    analyst_strong_buy=analyst_recommendations_data.get('strong_buy', 0),
                    analyst_buy=analyst_recommendations_data.get('buy', 0),
                    analyst_hold=analyst_recommendations_data.get('hold', 0),
                    analyst_sell=analyst_recommendations_data.get('sell', 0),
                    analyst_strong_sell=analyst_recommendations_data.get('strong_sell', 0)
                )
                
                # Copy ticker contributions from previous run
                prev_contributions = TickerContribution.objects.filter(analysis_run=latest_run)
                for contrib in prev_contributions:
                    TickerContribution.objects.create(
                        analysis_run=new_run,
                        ticker=contrib.ticker,
                        sentiment_score=contrib.sentiment_score,
                        market_cap_weight=contrib.market_cap_weight,
                        weighted_contribution=contrib.weighted_contribution,
                        articles_analyzed=contrib.articles_analyzed
                    )
                
                print(f"üìä Updated NASDAQ (^IXIC) Price: ${new_price:.2f} ({new_change:+.2f}%)")
                print(f"‚úÖ Created new run #{new_run.id} with updated price (sentiment unchanged)")
                print(f"‚è±Ô∏è  Completed in {time.time() - start_time:.1f} seconds")
                
                return new_run
            except Exception as e:
                print(f"‚ö†Ô∏è Error creating new run: {e}")
                return None
        else:
            print(f"‚ö†Ô∏è No previous analysis found - running full analysis")
            has_new_articles = True  # Force full analysis
    
    # Continue with full analysis if we have new articles
    print(f"\nüî¨ Proceeding with full sentiment analysis...")
    
    # Step 4: Collect ALL articles first (company + market news)
    print(f"\nüîç PHASE 2: Collecting all articles for single-batch analysis")

    # Collect company articles (top 10 per ticker)
    articles_to_process = []  # List of (article, ticker_obj, article_type, symbol) tuples
    ticker_article_counts = {}  # Track how many articles per ticker

    for symbol, weight in NASDAQ_TOP_20.items():
        ticker_obj = ticker_objects[symbol]
        news_articles = company_news_dict.get(symbol, [])

        if news_articles:
            top_articles = news_articles[:10]
            for article in top_articles:
                articles_to_process.append((article, ticker_obj, 'company', symbol))
            ticker_article_counts[symbol] = len(top_articles)
            print(f"  {symbol}: {len(top_articles)} articles")
        else:
            ticker_article_counts[symbol] = 0

    # Collect market news (top 20 most relevant)
    market_news = market_news_preview if 'market_news_preview' in locals() else fetch_general_market_news(finnhub_client)
    market_article_count = 0

    if market_news:
        top_market_news = market_news[:20]
        for article in top_market_news:
            articles_to_process.append((article, nasdaq_ticker, 'market', 'MARKET'))
        market_article_count = len(top_market_news)
        print(f"  MARKET: {len(top_market_news)} articles")

    total_articles = len(articles_to_process)
    print(f"\nüìä Total articles to process: {total_articles}")

    # Step 5: Single-batch analysis for ALL articles
    print(f"\nüî¨ PHASE 3: Analyzing ALL articles in single batch...")

    # Separate cached and uncached articles
    cached_results = {}  # article_index -> (article, ticker_obj, article_type, symbol, sentiment)
    uncached_indices = []  # List of indices that need analysis
    uncached_texts = []  # Corresponding texts for batch API call

    for idx, (article, ticker_obj, article_type, symbol) in enumerate(articles_to_process):
        headline = article.get('headline', '')
        summary = article.get('summary', '')
        article_hash = get_article_hash(headline, summary)

        cached_sentiment = get_cached_sentiment_from_db(article_hash)
        if cached_sentiment is not None:
            cached_results[idx] = (article, ticker_obj, article_type, symbol, cached_sentiment)
        else:
            uncached_indices.append(idx)
            uncached_texts.append(f"{headline}. {summary}")

    print(f"  ‚úì Cached: {len(cached_results)} | New: {len(uncached_indices)}")

    # Batch analyze all uncached articles in ONE API call
    new_sentiments = []
    if uncached_indices:
        print(f"  üî¨ Analyzing {len(uncached_indices)} new articles with FinBERT (single batch)...")
        new_sentiments = analyze_sentiment_finbert_batch(uncached_texts)
        print(f"  ‚úÖ Single batch analysis complete!")

    # Step 6: Process results with DIRECT WEIGHTING (Simplified Approach)
    print(f"\nüìà PHASE 4: Processing articles with direct market cap weighting")

    # General market news weight (for non-ticker-specific articles)
    GENERAL_MARKET_WEIGHT = 0.30

    # Create results mapping: index -> sentiment
    all_sentiments = {}
    for idx, (article, ticker_obj, article_type, symbol, sentiment) in cached_results.items():
        all_sentiments[idx] = sentiment

    for uncached_idx, sentiment in zip(uncached_indices, new_sentiments):
        all_sentiments[uncached_idx] = sentiment

    # Accumulate weighted scores directly (no per-ticker averaging)
    total_weighted_contribution = 0.0
    article_count = 0

    # Track for database saving and reporting
    ticker_contributions = {}
    all_company_articles = []
    market_articles_data = []
    ticker_article_counts = {}

    # Initialize ticker contribution tracking
    for symbol, weight in NASDAQ_TOP_20.items():
        ticker_contributions[symbol] = {
            'sentiment': 0.0,
            'weight': weight,
            'contribution': 0.0,
            'articles_count': 0,
            'articles_data': [],
            'weighted_sum': 0.0  # Track weighted sum for averaging later
        }

    # Process ALL articles with direct weighting
    print(f"  Processing {len(articles_to_process)} articles with direct market cap weighting...")

    skipped_timeout_articles = 0

    for idx, (article, ticker_obj, article_type, symbol) in enumerate(articles_to_process):
        sentiment = all_sentiments.get(idx, 0.0)

        # Skip articles that timed out (sentiment = None)
        if sentiment is None:
            skipped_timeout_articles += 1
            continue  # ‚úÖ Don't process or save timeout articles

        if symbol == 'MARKET':
            # Market news: use fixed weight
            article_data = analyze_article_sentiment(article, nasdaq_ticker, 'market', base_sentiment=sentiment)
            weighted_contribution = article_data['article_score'] * GENERAL_MARKET_WEIGHT
            market_articles_data.append(article_data)

        else:
            # Company news: use market cap weight
            weight = NASDAQ_TOP_20.get(symbol, 0.0)
            article_data = analyze_article_sentiment(article, ticker_obj, 'company', base_sentiment=sentiment)
            weighted_contribution = article_data['article_score'] * weight
            all_company_articles.append(article_data)

            # Track for this ticker (for database/reporting)
            ticker_contributions[symbol]['articles_data'].append(article_data)
            ticker_contributions[symbol]['articles_count'] += 1
            ticker_contributions[symbol]['weighted_sum'] += article_data['article_score']

        # Accumulate total
        total_weighted_contribution += weighted_contribution
        article_count += 1

    if skipped_timeout_articles > 0:
        print(f"  ‚ö†Ô∏è  Skipped {skipped_timeout_articles} articles due to API timeout (will retry next run)")

    # Calculate average sentiment per ticker (for reporting and TickerContribution table)
    for symbol, contrib in ticker_contributions.items():
        if contrib['articles_count'] > 0:
            contrib['sentiment'] = contrib['weighted_sum'] / contrib['articles_count']
            contrib['contribution'] = contrib['sentiment'] * contrib['weight']
            print(f"  {symbol} ({contrib['weight']:.1%}): {contrib['sentiment']:+.2f} | Articles: {contrib['articles_count']}")

    # Report market news
    if market_articles_data:
        market_avg = sum(a['article_score'] for a in market_articles_data) / len(market_articles_data)
        market_weighted = market_avg * GENERAL_MARKET_WEIGHT
        print(f"  MARKET ({GENERAL_MARKET_WEIGHT:.1%}): {market_avg:+.2f} | Articles: {len(market_articles_data)}")

    print(f"\nüìä Total articles processed: {article_count}")
    print(f"üìä Company articles: {len(all_company_articles)}")
    print(f"üìä Market articles: {len(market_articles_data)}")

    # Step 7: Calculate news composite with decay + new articles (SIMPLIFIED)
    # Load previous news_composite score and apply decay
    latest_run = AnalysisRun.objects.filter(ticker=nasdaq_ticker).order_by('-timestamp').first()

    if latest_run and latest_run.avg_base_sentiment is not None:
        previous_news_composite = latest_run.avg_base_sentiment

        # Calculate time elapsed since last update (in minutes)
        time_elapsed = (timezone.now() - latest_run.timestamp).total_seconds() / 60
        minutes_elapsed = max(1, int(time_elapsed))  # At least 1 minute

        # Apply decay to previous score
        decayed_score = apply_news_decay(previous_news_composite, minutes_elapsed)

        print(f"\nüìâ Applying decay to previous news score:")
        print(f"   Previous score: {previous_news_composite:+.2f}")
        print(f"   Minutes elapsed: {minutes_elapsed}")
        print(f"   Decayed score: {decayed_score:+.2f}")
    else:
        # First run - no previous score
        decayed_score = 0.0
        print(f"\nüì∞ First run - starting with score = 0")

    # Calculate new article impact (DIRECT WEIGHTING - no 70/30 split)
    # Average the weighted contributions across all articles
    new_article_impact = total_weighted_contribution / article_count if article_count > 0 else 0.0

    # Cap per-run impact at ¬±25 to prevent single-run spikes
    new_article_impact = max(-25, min(25, new_article_impact))

    # Combine decayed score + new article impact
    news_composite = decayed_score + new_article_impact

    # Final cap at -100/+100
    news_composite = max(-100, min(100, news_composite))

    print(f"\nüì∞ News Composite Calculation (Simplified Direct Weighting):")
    print(f"   Decayed previous score: {decayed_score:+.2f}")
    print(f"   New article impact (averaged): {new_article_impact:+.2f}")
    print(f"   Per-run cap applied: ¬±25")
    print(f"   Combined (before final cap): {decayed_score + new_article_impact:+.2f}")
    print(f"   Final news_composite: {news_composite:+.2f}")

    # Step 8: Get NASDAQ index price and OHLCV data from Yahoo Finance (NASDAQ Composite Index)
    try:
        print(f"\nüìä Fetching real-time OHLCV from Yahoo Finance...")
        ohlcv = fetch_latest_ohlcv_from_yfinance(symbol='^IXIC', interval='1m')

        if ohlcv:
            index_price = Decimal(str(ohlcv['close']))
            price_open = Decimal(str(ohlcv['open']))
            price_high = Decimal(str(ohlcv['high']))
            price_low = Decimal(str(ohlcv['low']))
            volume = ohlcv['volume']

            # Calculate price change percentage
            price_change = ((ohlcv['close'] - ohlcv['open']) / ohlcv['open'] * 100) if ohlcv['open'] != 0 else 0
        else:
            raise Exception("No OHLCV data from Yahoo Finance")

    except Exception as e:
        print(f"\n‚ö†Ô∏è Could not fetch OHLCV from Yahoo Finance: {e}")
        print(f"  Falling back to Finnhub quote...")
        try:
            quote = finnhub_client.quote('^IXIC')
            index_price = Decimal(str(quote['c']))
            price_open = Decimal(str(quote.get('o', quote['c'])))
            price_high = Decimal(str(quote.get('h', quote['c'])))
            price_low = Decimal(str(quote.get('l', quote['c'])))
            price_change = quote.get('dp', 0)
            volume = None
        except Exception as e2:
            print(f"\n‚ö†Ô∏è Finnhub fallback also failed: {e2}")
            index_price = Decimal('0.00')
            price_open = Decimal('0.00')
            price_high = Decimal('0.00')
            price_low = Decimal('0.00')
            price_change = 0.0
            volume = None
    
    # Step 9: Calculate technical indicators from OHLCV data
    # Fetch fresh OHLCV data and calculate indicators on every run
    technical_indicators, symbol_used = fetch_indicators_with_fallback(
        symbols=INDICATOR_SYMBOLS,
        resolution=INDICATOR_RESOLUTION,
        hours_back=INDICATOR_LOOKBACK_HOURS,
        config=INDICATOR_PERIODS
    )

    # Step 9.5: Calculate technical composite score
    technical_composite_score = calculate_technical_composite_score(technical_indicators)
    print(f"\nüìä Technical Indicators Composite Score: {technical_composite_score:+.2f}")

    # Step 9.6: Fetch VXN (NASDAQ-100 Volatility Index)
    vxn_value = fetch_vxn_price()

    # Step 10: Calculate FINAL composite score (4-factor model)
    # Weighting optimized for market movement prediction:
    # - News: Most impactful for immediate market reactions (35%)
    # - Technical: Price action and momentum signals (25%)
    # - Social Media: Retail sentiment and momentum (20%)
    # - Analyst Recommendations: Professional institutional outlook (20%)
    NEWS_WEIGHT = 0.35
    SOCIAL_WEIGHT = 0.20
    TECHNICAL_WEIGHT = 0.25
    ANALYST_WEIGHT = 0.20

    final_composite_score = (
        news_composite * NEWS_WEIGHT +
        reddit_sentiment * SOCIAL_WEIGHT +
        technical_composite_score * TECHNICAL_WEIGHT +
        analyst_recommendations_score * ANALYST_WEIGHT
    )

    print(f"\n{'='*80}")
    print(f"üéØ FINAL NASDAQ COMPOSITE SENTIMENT SCORE: {final_composite_score:+.2f}")
    print(f"{'='*80}")
    print(f"   News Sentiment:            {news_composite:+.2f} √ó {NEWS_WEIGHT:.0%} = {news_composite * NEWS_WEIGHT:+.2f}")
    print(f"   Social Media (Reddit):     {reddit_sentiment:+.2f} √ó {SOCIAL_WEIGHT:.0%} = {reddit_sentiment * SOCIAL_WEIGHT:+.2f}")
    print(f"   Technical Indicators:      {technical_composite_score:+.2f} √ó {TECHNICAL_WEIGHT:.0%} = {technical_composite_score * TECHNICAL_WEIGHT:+.2f}")
    print(f"   Analyst Recommendations:   {analyst_recommendations_score:+.2f} √ó {ANALYST_WEIGHT:.0%} = {analyst_recommendations_score * ANALYST_WEIGHT:+.2f}")
    print(f"{'='*80}")

    # Step 11: Save to database
    print(f"\nüíæ Saving analysis to database...")

    with transaction.atomic():
        # Calculate averages for component scores
        all_articles = all_company_articles + market_articles_data
        total_articles = len(all_articles)
        cached_count = sum(1 for a in all_articles if a['is_cached'])
        new_count = total_articles - cached_count

        # NOTE: avg_base_sentiment now stores the news_composite score (with decay)
        # This is the score that will be used for decay calculations in the next run
        avg_surprise = sum(a['surprise_factor'] for a in all_articles) / total_articles if total_articles else 1.0
        # Novelty and recency may be None (feature deprecated); treat None as neutral defaults
        avg_novelty = (
            sum((a['novelty_score'] if a['novelty_score'] is not None else 1.0) for a in all_articles) / total_articles
            if total_articles else 1.0
        )
        avg_credibility = sum(a['source_credibility'] for a in all_articles) / total_articles if total_articles else 0.5
        avg_recency = (
            sum((a['recency_weight'] if a['recency_weight'] is not None else 0.5) for a in all_articles) / total_articles
            if total_articles else 0.5
        )

        # Create AnalysisRun with OHLCV and technical indicators
        analysis_run = AnalysisRun.objects.create(
            ticker=nasdaq_ticker,
            composite_score=float(final_composite_score),
            avg_base_sentiment=news_composite,  # Save the actual news_composite (with decay)
            avg_surprise_factor=avg_surprise,
            avg_novelty=avg_novelty,
            avg_source_credibility=avg_credibility,
            avg_recency_weight=avg_recency,
            # OHLCV data (from Yahoo Finance)
            stock_price=index_price,
            price_open=price_open,
            price_high=price_high,
            price_low=price_low,
            price_change_percent=price_change,
            volume=volume,
            articles_analyzed=total_articles,
            cached_articles=cached_count,
            new_articles=new_count,
            # Technical indicators
            rsi_14=technical_indicators.get('rsi_14'),
            macd=technical_indicators.get('macd'),
            macd_signal=technical_indicators.get('macd_signal'),
            macd_histogram=technical_indicators.get('macd_histogram'),
            bb_upper=technical_indicators.get('bb_upper'),
            bb_middle=technical_indicators.get('bb_middle'),
            bb_lower=technical_indicators.get('bb_lower'),
            sma_20=technical_indicators.get('sma_20'),
            sma_50=technical_indicators.get('sma_50'),
            ema_9=technical_indicators.get('ema_9'),
            ema_20=technical_indicators.get('ema_20'),
            stoch_k=technical_indicators.get('stoch_k'),
            stoch_d=technical_indicators.get('stoch_d'),
            williams_r=technical_indicators.get('williams_r'),
            atr_14=technical_indicators.get('atr_14'),
            qqq_price=technical_indicators.get('qqq_price'),
            # VXN (NASDAQ-100 Volatility Index)
            vxn_index=vxn_value,
            # Reddit sentiment
            reddit_sentiment=reddit_sentiment,
            reddit_posts_analyzed=reddit_analysis_data.get('posts_analyzed', 0),
            reddit_comments_analyzed=reddit_analysis_data.get('comments_analyzed', 0),
            # Technical composite score
            technical_composite_score=technical_composite_score,
            # Analyst recommendations
            analyst_recommendations_score=analyst_recommendations_score,
            analyst_recommendations_count=analyst_recommendations_data.get('total_recommendations', 0),
            analyst_strong_buy=analyst_recommendations_data.get('strong_buy', 0),
            analyst_buy=analyst_recommendations_data.get('buy', 0),
            analyst_hold=analyst_recommendations_data.get('hold', 0),
            analyst_sell=analyst_recommendations_data.get('sell', 0),
            analyst_strong_sell=analyst_recommendations_data.get('strong_sell', 0)
        )
        
        print(f"‚úì Created AnalysisRun #{analysis_run.id}")
        
        # Save ticker contributions
        for symbol, contrib in ticker_contributions.items():
            if contrib['articles_count'] > 0:
                TickerContribution.objects.create(
                    analysis_run=analysis_run,
                    ticker=ticker_objects[symbol],
                    sentiment_score=contrib['sentiment'],
                    market_cap_weight=contrib['weight'],
                    weighted_contribution=contrib['contribution'],
                    articles_analyzed=contrib['articles_count']
                )
        
        print(f"‚úì Saved {len(ticker_contributions)} ticker contributions")
        
        # Save all articles
        for article_data in all_articles:
            NewsArticle.objects.update_or_create(
                article_hash=article_data['article_hash'],
                defaults={
                    'ticker': nasdaq_ticker if article_data['article_type'] == 'market' else ticker_objects.get(article_data.get('ticker_symbol', '^IXIC'), nasdaq_ticker),
                    'analysis_run': analysis_run,
                    'headline': article_data['headline'],
                    'summary': article_data['summary'],
                    'source': article_data['source'],
                    'url': article_data['url'],
                    'published_at': article_data['published_at'],
                    'article_type': article_data['article_type'],
                    'base_sentiment': article_data['base_sentiment'],
                    'surprise_factor': article_data['surprise_factor'],
                    'novelty_score': article_data['novelty_score'],
                    'source_credibility': article_data['source_credibility'],
                    'recency_weight': article_data['recency_weight'],
                    'article_score': article_data['article_score'],
                    'weighted_contribution': article_data['article_score'] / total_articles if total_articles else 0,
                    'is_analyzed': True,
                    'sentiment_cached': article_data['is_cached']
                }
            )
        
        print(f"‚úì Saved {total_articles} news articles")
    
    elapsed_time = time.time() - start_time
    print(f"\n‚úÖ Analysis complete in {elapsed_time:.1f} seconds")
    print(f"üìä View results: http://localhost:8000/admin/api/analysisrun/{analysis_run.id}/change/")
    
    return analysis_run


# ============================================================================
# DJANGO COMMAND
# ============================================================================

class Command(BaseCommand):
    help = 'Run NASDAQ composite sentiment analysis using hybrid approach'
    
    def add_arguments(self, parser):
        parser.add_argument(
            '--interval',
            type=int,
            default=300,
            help='Update interval in seconds (default: 300 = 5 minutes)'
        )
        parser.add_argument(
            '--once',
            action='store_true',
            help='Run analysis once and exit (don\'t monitor continuously)'
        )
    
    def handle(self, *args, **options):
        interval = options['interval']
        run_once = options['once']

        # Check if market is open (can be skipped with SKIP_MARKET_HOURS_CHECK env var)
        skip_market_check = os.environ.get('SKIP_MARKET_HOURS_CHECK', 'False') == 'True'

        if not skip_market_check:
            market_open, reason = is_market_open()
            if not market_open:
                self.stdout.write(self.style.WARNING(
                    f'‚è∏Ô∏è  Market Closed - Skipping Run\n'
                    f'   Reason: {reason}\n'
                    f'   Current time: {get_market_status()["current_time_ct"]}'
                ))
                return

        # Validate API keys
        if not FINNHUB_API_KEY:
            self.stdout.write(self.style.ERROR(
                '‚ùå FINNHUB_API_KEY not set! Set it in .env file.'
            ))
            return
        
        if not HUGGINGFACE_API_KEY:
            self.stdout.write(self.style.ERROR(
                '‚ùå HUGGINGFACE_API_KEY not set! Set it in .env file.'
            ))
            return
        
        # Initialize Finnhub client
        finnhub_client = finnhub.Client(api_key=FINNHUB_API_KEY)
        
        self.stdout.write(self.style.SUCCESS(
            '\nüöÄ NASDAQ Composite Sentiment Tracker - Hybrid Approach'
        ))
        self.stdout.write(f'üìä Tracking {len(NASDAQ_TOP_20)} top NASDAQ stocks')
        self.stdout.write(f'üîÑ Company News Weight: {SENTIMENT_WEIGHTS["company_news"]:.0%}')
        self.stdout.write(f'üì° Market News Weight: {SENTIMENT_WEIGHTS["market_news"]:.0%}')
        self.stdout.write('üíæ All data saved to database\n')
        
        if run_once:
            # Run once
            result = run_nasdaq_composite_analysis(finnhub_client)
            if result:
                # Handle both dict (price-only update) and AnalysisRun object
                if isinstance(result, dict):
                    self.stdout.write(f"\nüìä Current Score: {result['composite_score']:+.2f}")
                    self.stdout.write(f"üí∞ Stock Price: ${result['stock_price']:.2f} ({result['price_change']:+.2f}%)")
                    self.interpret_score(result['composite_score'])
                else:
                    self.interpret_score(result.composite_score)
        else:
            # Run continuously
            self.stdout.write(f'‚è±Ô∏è  Update interval: {interval} seconds')
            self.stdout.write('‚å®Ô∏è  Press Ctrl+C to stop\n')
            
            iteration = 0
            
            try:
                while True:
                    iteration += 1
                    self.stdout.write(f"\n{'>'*30} ITERATION {iteration} {'<'*30}")
                    
                    result = run_nasdaq_composite_analysis(finnhub_client)
                    if result:
                        # Handle both dict (price-only update) and AnalysisRun object
                        if isinstance(result, dict):
                            self.stdout.write(f"\nüìä Current Score: {result['composite_score']:+.2f}")
                            self.stdout.write(f"üí∞ Stock Price: ${result['stock_price']:.2f} ({result['price_change']:+.2f}%)")
                            self.interpret_score(result['composite_score'])
                        else:
                            self.interpret_score(result.composite_score)
                    
                    self.stdout.write(f"\nüí§ Next update in {interval} seconds...")
                    time.sleep(interval)
                    
            except KeyboardInterrupt:
                self.stdout.write(self.style.WARNING(
                    f'\n\n‚õî Monitoring stopped by user.'
                ))
                self.stdout.write(f'üìä Total iterations: {iteration}')
                self.stdout.write(f'üì∞ Unique articles seen: {len(seen_articles)}')
    
    def interpret_score(self, score):
        """Interpret and display the sentiment score"""
        if score > 50:
            sentiment = self.style.SUCCESS("STRONGLY BULLISH üöÄ")
        elif score > 20:
            sentiment = self.style.SUCCESS("BULLISH üìà")
        elif score > -20:
            sentiment = self.style.WARNING("NEUTRAL ‚û°Ô∏è")
        elif score > -50:
            sentiment = self.style.ERROR("BEARISH üìâ")
        else:
            sentiment = self.style.ERROR("STRONGLY BEARISH üîª")
        
        self.stdout.write(f"\n{'='*80}")
        self.stdout.write(f"Current NASDAQ Sentiment: {sentiment}")
        self.stdout.write(f"Score: {score:+.2f}/100")
        self.stdout.write(f"{'='*80}")

